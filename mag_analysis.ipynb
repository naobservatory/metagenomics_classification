{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAG Analysis\n",
    "\n",
    "Analysing assembled and binned metagenomes generated using the [nf-core/mag](https://github.com/PhilPalmer/mag/tree/genomad) pipeline on the [Rothman dataset](https://doi.org/10.1128/AEM.01448-21) with samples from the [HTP site](https://en.wikipedia.org/wiki/Hyperion_sewage_treatment_plant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Change\n",
    "########\n",
    "# TODO: Upload directory to AWS S3 and add code to download the data if needed\n",
    "data_dir = '../mag/results_rothman_htp'\n",
    "\n",
    "##############\n",
    "# Don't change\n",
    "##############\n",
    "\n",
    "# Assembly data\n",
    "# spades_dir = f'{data_dir}/Assembly/SPAdes'\n",
    "# spades_fa_gz = f'{spades_dir}/SPAdes-group-HTP_scaffolds.fasta.gz'\n",
    "\n",
    "# Genome Binning data\n",
    "binner = 'MaxBin2'\n",
    "busco_tsv = f'{data_dir}/GenomeBinning/QC/busco_summary.tsv'\n",
    "quast_tsv = f'{data_dir}/GenomeBinning/QC/quast_summary.tsv'\n",
    "bin_tsv = f'{data_dir}/GenomeBinning/bin_summary.tsv'\n",
    "bin_dir = f'{data_dir}/GenomeBinning/{binner}'\n",
    "\n",
    "# Taxonomy data\n",
    "genomad_dir = f'{data_dir}/Taxonomy/geNomad'\n",
    "genomad_tsv = f'{genomad_dir}/group-HTP/SPAdes-group-HTP_scaffolds_aggregated_classification/SPAdes-group-HTP_scaffolds_aggregated_classification.tsv'\n",
    "gtdbtk_tsv = f'{data_dir}/Taxonomy/GTDB-Tk/gtdbtk_summary.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def decompress_gz(gz_file):\n",
    "    \"\"\"\n",
    "    Decompress a gzipped file.\n",
    "    \"\"\"\n",
    "    file = gz_file.replace('.gz', '')\n",
    "    if os.path.exists(gz_file) and not os.path.exists(file):\n",
    "        !gzip -dk {gz_file}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load each contig with any relevant information eg the bin/MAG and coverage etc.\n",
    "\n",
    "Link the infomation at the sample, assembly, MAG and taxonomic levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemblies\n",
    "\n",
    "[metaSPAdes](http://cab.spbu.ru/software/spades/) was used for the assembly and outputs the contigs i.e. merged reads as well as the scaffolds i.e. contigs that have been merged together\n",
    "\n",
    "The commented code below loads the contigs, however, we will be skipping this step because the same information will be loaded using the MAGs instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decompress the FASTA file (if it hasn't already been decompressed)\n",
    "# spades_fa = decompress_gz(spades_fa_gz)\n",
    "\n",
    "# # Load the FASTA seqs and save them as a dict in the following format {'node_id': ['length', 'cov', 'seq']}\n",
    "# seqs_dict = {int(record.id.split('_')[1]): [int(record.id.split('_')[3]), float(record.id.split('_')[5]), str(record.seq)] for record in SeqIO.parse(spades_fa, 'fasta')}\n",
    "\n",
    "# # Create dataframe containing contigs info\n",
    "# seqs_df = pd.DataFrame.from_dict(seqs_dict, orient='index', columns=['length', 'coverage', 'seq'])\n",
    "\n",
    "# seqs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the bins/MAGs\n",
    "\n",
    "- Two tools were used to perform metagenome binning to generate metagenome assembled genomes (MAGs) - [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/) and [MaxBin2](https://sourceforge.net/projects/maxbin2/)\n",
    "\n",
    "- And two tools were used to check for quality control (QC) of the genome bins - [Busco](https://busco.ezlab.org/) and [Quast](http://quast.sourceforge.net/quast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look the summary information of the bins/MAGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_df = pd.read_csv(bin_tsv, sep='\\t')\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all of the sample IDs\n",
    "samples = [col.split(' ')[1] for col in bin_df.columns if 'Depth ' in col]\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the tools show that MaxBin2 generated more genome bins and a higher % completeness than MetaBAT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busco_df = pd.read_csv(busco_tsv, sep='\\t')\n",
    "busco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quast_df = pd.read_csv(quast_tsv, sep='\\t')\n",
    "quast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will therefore use the MaxBin2 results for the rest of the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bin files\n",
    "bin_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(bin_dir):\n",
    "    bin_files.extend([os.path.join(dirpath, file) for file in filenames])\n",
    "\n",
    "bin_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dfs = []\n",
    "\n",
    "for bin_file_gz in bin_files:\n",
    "\n",
    "    # Load the bin file and save as a dataframe\n",
    "    bin_file = decompress_gz(bin_file_gz)\n",
    "    bin_dict = {int(record.id.split('_')[1]): [int(record.id.split('_')[3]), float(record.id.split('_')[5]), str(record.seq)] for record in SeqIO.parse(bin_file, 'fasta')}\n",
    "    bin_df = pd.DataFrame.from_dict(bin_dict, orient='index', columns=['length', 'coverage', 'seq'])\n",
    "\n",
    "    # Add the bin ID to the dataframe\n",
    "    bin_df['bin_id'] = bin_file.split('/')[-1].split('.')[1]\n",
    "\n",
    "    # Add the dataframe to the list\n",
    "    bin_dfs.append(bin_df)\n",
    "\n",
    "# Concatenate all of the dataframes\n",
    "bin_df = pd.concat(bin_dfs).sort_index()\n",
    "bin_df = bin_df[~bin_df.index.duplicated(keep='first')]\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virus classification\n",
    "\n",
    "Let's load the virus classifications predicted using [geNomad](https://github.com/apcamargo/genomad) and combine this with our existing data for the contigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the virus classification data\n",
    "vir_df = pd.read_csv(genomad_tsv, sep='\\t')\n",
    "vir_df.index = vir_df.seq_name.str.split('_').str[1].astype(int)\n",
    "vir_df = vir_df.drop('seq_name', axis=1)\n",
    "vir_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the binning and taxonomy dataframes\n",
    "df = pd.merge(bin_df, vir_df, left_index=True, right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomic classification of binned genomes\n",
    "Load the GTDB-Tk summary table (see [column descriptions](https://ecogenomics.github.io/GTDBTk/files/summary.tsv.html)) and combine with our existng information for the contigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GTDB-Tk classification data\n",
    "gtdbtk_df = pd.read_csv(gtdbtk_tsv, sep='\\t')\n",
    "\n",
    "# Filter the GTDB-Tk dataframe\n",
    "gtdbtk_df = gtdbtk_df[gtdbtk_df.user_genome.str.contains(binner)]\n",
    "cols = ['user_genome', 'classification', 'classification_method', 'other_related_references(genome_id,species_name,radius,ANI,AF)', 'msa_percent', 'red_value', 'warnings']\n",
    "gtdbtk_df = gtdbtk_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the GTDB-Tk classification data to the main dataframe\n",
    "gtdbtk_df['bin_id'] = gtdbtk_df.user_genome.str.split('.').str[1]\n",
    "gtdbtk_df = gtdbtk_df.drop('user_genome', axis=1)\n",
    "df = pd.merge(df, gtdbtk_df, on='bin_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomic classification of trimmed reads\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get read counts for the MAGs\n",
    "\n",
    "TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c71d42a0e06c490c9858db74c56ab08eaac94618dde2a8910e248a9491f2839f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
