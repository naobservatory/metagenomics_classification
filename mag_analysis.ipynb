{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAG Analysis\n",
    "\n",
    "Data from the [Rothman study](https://doi.org/10.1128/AEM.01448-21) corresponding to (unenriched) samples from the [HTP site](https://en.wikipedia.org/wiki/Hyperion_sewage_treatment_plant) was downloaded from the [ENA](https://www.ebi.ac.uk/ena/browser/view/prjna729801) and processed using the [nf-core/mag](https://github.com/PhilPalmer/mag/tree/genomad) pipeline to generate assembled and binned metagenomes (as shown by the image below). Here, we will analyse the results generated by the pipeline. First, we will load and aggregate the pipeline results, explaining the steps along the way. Then, we will use the aggregated results to answer some key questions about the data such as what organisms are present in the samples and how do the taxonomic classifications generated using the reads and the assembled genomes compare.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nf-core/mag/master/docs/images/mag_workflow.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "from Bio import SeqIO\n",
    "from matplotlib import rcParams\n",
    "# from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Change\n",
    "########\n",
    "# TODO: Upload directory to AWS S3 and add code to download the data if needed\n",
    "data_dir = '../mag/results_rothman_htp'\n",
    "\n",
    "##############\n",
    "# Don't change\n",
    "##############\n",
    "\n",
    "# QC\n",
    "fastqc_yaml = f'{data_dir}/multiqc/multiqc_data/multiqc_fastqc.yaml'\n",
    "\n",
    "# Assembly data\n",
    "bam_dir = f'{data_dir}/Assembly/SPAdes/QC/group-HTP'\n",
    "\n",
    "# Genome Binning data\n",
    "binner = 'MaxBin2'\n",
    "busco_tsv = f'{data_dir}/GenomeBinning/QC/busco_summary.tsv'\n",
    "quast_tsv = f'{data_dir}/GenomeBinning/QC/quast_summary.tsv'\n",
    "bin_tsv = f'{data_dir}/GenomeBinning/bin_summary.tsv'\n",
    "bin_dir = f'{data_dir}/GenomeBinning/{binner}'\n",
    "\n",
    "# Taxonomy data\n",
    "kraken_dir = f'{data_dir}/Taxonomy/kraken2'\n",
    "genomad_dir = f'{data_dir}/Taxonomy/geNomad'\n",
    "genomad_class = f'{genomad_dir}/group-HTP/SPAdes-group-HTP_scaffolds_aggregated_classification/SPAdes-group-HTP_scaffolds_aggregated_classification.tsv'\n",
    "genomad_tax = f'{genomad_dir}/group-HTP/SPAdes-group-HTP_scaffolds_annotate/SPAdes-group-HTP_scaffolds_taxonomy.tsv'\n",
    "gtdbtk_tsv = f'{data_dir}/Taxonomy/GTDB-Tk/gtdbtk_summary.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def decompress_gz(gz_file):\n",
    "    \"\"\"\n",
    "    Decompress a gzipped file.\n",
    "    \"\"\"\n",
    "    file = gz_file.replace('.gz', '')\n",
    "    if os.path.exists(gz_file) and not os.path.exists(file):\n",
    "        !gzip -dk {gz_file}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will load the data from the key steps of the pipeline in the rough order they ran.\n",
    "\n",
    "We will load each contig and associate it with any relevant information such as the length, bin/MAG and coverage etc. As we have information at different levels (sample, assembly, MAG and taxonomic levels) we will use IDs to link information in different dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Quality Control (QC)\n",
    "The first step of the pipeline was to perform QC on the raw reads. Here we will load the data from [FastQC](https://github.com/s-andrews/FastQC)/[MultiQC](https://github.com/ewels/MultiQC) to obtain the sample names and the total number of raw reads per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read the QC data\n",
    "with open(fastqc_yaml, 'r') as f:\n",
    "    fastqc_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Get the sample IDs and the total number of raw reads\n",
    "sample_read_counts = {k: fastqc_data[k]['Total Sequences'] for k in fastqc_data.keys()}\n",
    "sample_read_counts = {k.replace('_1',''): int(sample_read_counts[k] + sample_read_counts[k.replace('_1', '_2')]) for k in sample_read_counts.keys() if '_1' in k}\n",
    "samples = list(sample_read_counts.keys())\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assemblies\n",
    "\n",
    "[metaSPAdes](http://cab.spbu.ru/software/spades/) was used for the assembly i.e. merging of reads.\n",
    "\n",
    "(We will skip loading the contigs here because instead, we will load the MAGs (in the next step) which includes the same information for the contigs and additional information such as the bin).\n",
    "\n",
    "#### Getting read counts for the contigs and samples\n",
    "\n",
    "Here we will generate a count matrix i.e. a file mapping contig IDs (rows) and sample IDs (columns).\n",
    "\n",
    "The nf-core/mag pipeline already maps the reads back to the contigs using [bowtie2](https://github.com/BenLangmead/bowtie2) (see [code](https://github.com/nf-core/mag/blob/master/modules/local/bowtie2_assembly_align.nf#L23-L30)). Therefore, we can use [samtools](http://www.htslib.org/) to get the read counts for each contig and sample (see [example](https://github.com/edamame-course/Metagenome/blob/master/2018-06-29-counting-abundance-with-mapped-reads.md#option-1-count-each-contigs)). The third column in the tab-seperated output from [`samtools idxstats`](http://www.htslib.org/doc/samtools-idxstats.html) is the number of reads mapped to the contig which we'll use to generate the count matrix.\n",
    "\n",
    "TODO: Explain how bowtie2 maps reads to contigs i.e. how does it check for matches\n",
    "\n",
    "##### How does bowtie2 align reads to contigs?\n",
    "Bowtie2 suppports different modes of alignment. The default mode (used by the nf-core/mag pipeline) is \"end-to-end\" alignment which uses all characters in the read for alignment, e.g.:\n",
    "```\n",
    "Alignment:\n",
    "  Read:      GACTGGGCGATCTCGACTTCG\n",
    "             |||||  |||||||||| |||\n",
    "  Reference: GACTG--CGATCTCGACATCG\n",
    "```\n",
    "\n",
    "An alignment score is used to quantify how similiar the read is to the aligned contig sequence. The higher the score, the more similiar the read is to the contig. A mismatched base at a high-quality position in the read receives a penalty of -6 by default. A length-2 read gap receives a penalty of -11 by default (-5 for the gap open, -3 for the first extension, -3 for the second extension). Thus, in end-to-end alignment mode, if the read is 150 bp long and it matches the reference exactly except for one mismatch at a high-quality position and one length-2 read gap, then the overall score is -(6 + 11) = -17. The best possible alignment score in end-to-end mode is 0, which happens when there are no differences between the read and the reference\n",
    "\n",
    "A \"match\" is determined if the alignment score is no less than the minimum score threshold. In end-to-end alignment mode, the default minimum score threshold is `-0.6 + -0.6 * L`, where `L` is the read length. For example, using a read length of 150 bp, the minimum score threshold is `-0.6 + -0.6 * 150 = -90.6`. Therefore, if the alignment score is no less than -90 (e.g. there are fewer than 15 high-quality mismatches and no gaps), then the read is considered a \"match\".\n",
    "\n",
    "<!--\n",
    "Bowtie2 uses a heuristic to determine whether a read is \"good enough\" to be considered a \"match\". The heuristic is based on the read length and the alignment score. The alignment score is the sum of the scores of the matching bases and gaps. The alignment score is compared to a minimum score threshold. If the alignment score is no less than the minimum score threshold, then the read is considered \"good enough\" to be considered aligned. The minimum score threshold is configurable and is expressed as a function of the read length. In end-to-end alignment mode, the default minimum score threshold is `-0.6 + -0.6 * L`, where `L` is the read length. For example, using a read length of 150 bp, the minimum score threshold is `-0.6 + -0.6 * 150 = -90.6`. Therefore, if the alignment score is no less than -90 (e.g. there are fewer than 15 high-quality mismatches), then the read is considered a \"match\" and the contig is considered to have a read mapped to it. Information from both of the paired reads is also used in the alignment.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get absolute path for bam_dir and generate a list of all of the bams\n",
    "bam_dir = os.path.abspath(bam_dir)\n",
    "bams = [f'{bam_dir}/{bam}' for bam in os.listdir(bam_dir) if bam.endswith('.bam')]\n",
    "\n",
    "# Check if samtools is installed locally, if not, use the docker container from the nf-core/mag pipeline to run samtools\n",
    "if shutil.which('samtools'):\n",
    "    samtools_cmd = 'samtools'\n",
    "else:\n",
    "    samtools_cmd = f'docker run -v {bam_dir}:{bam_dir} -w {bam_dir} quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0 samtools'\n",
    "\n",
    "# Run samtools idxstats on all of the bams\n",
    "for bam in bams:\n",
    "    sample = bam.split('/')[-1].split('.')[0].split('-')[-1]\n",
    "    if not os.path.exists(f'{bam_dir}/{sample}_idxstats.txt'):\n",
    "        # TODO: Use a method that assigns each read to the single best matching contig only and doesn't count reads multiple times\n",
    "        !{samtools_cmd} idxstats {bam} > {bam_dir}/{sample}_idxstats.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load and concatenate the idxstats files\n",
    "idxstat_cols = ['contig', 'length', 'num_mapped_reads', 'num_unmapped_reads']\n",
    "idxstat_dfs = []\n",
    "\n",
    "# Get a list of all of the idxstats files\n",
    "idxstats = [f'{bam_dir}/{idxstat}' for idxstat in os.listdir(bam_dir) if idxstat.endswith('_idxstats.txt')]\n",
    "\n",
    "# Read in the idxstats files\n",
    "for idxstat in idxstats:\n",
    "    sample = idxstat.split('/')[-1].split('_')[0]\n",
    "    idxstat_df = pd.read_csv(idxstat, sep='\\t', header=None, names=idxstat_cols)\n",
    "    idxstat_df['sample_id'] = sample\n",
    "    idxstat_dfs.append(idxstat_df)\n",
    "\n",
    "# Combine the idxstats dataframes into a single dataframe\n",
    "counts_df = pd.concat(idxstat_dfs, axis=0)\n",
    "counts_df.contig = counts_df.contig.str.split('_').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reformat the dataframe so that the contigs are the index and the samples are the columns\n",
    "counts_df = counts_df.pivot(index='contig', columns='sample_id', values='num_mapped_reads')\n",
    "\n",
    "# Sort the dataframe\n",
    "counts_df = counts_df[~counts_df.index.isnull()]\n",
    "counts_df.index = counts_df.index.astype(int)\n",
    "counts_df = counts_df.sort_index()\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Genome binning\n",
    "\n",
    "Here we will load the bins/MAGs\n",
    "\n",
    "- Two tools were used to perform metagenome binning to generate metagenome assembled genomes (MAGs) - [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/) and [MaxBin2](https://sourceforge.net/projects/maxbin2/)\n",
    "\n",
    "- And two tools were used to check for quality control (QC) of the genome bins - [Busco](https://busco.ezlab.org/) and [Quast](http://quast.sourceforge.net/quast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's look the summary information of the bins/MAGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "bin_summary_df = pd.read_csv(bin_tsv, sep='\\t', index_col=0)\n",
    "bin_summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results from the tools show that MaxBin2 generated more genome bins and a higher % completeness than MetaBAT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "busco_df = pd.read_csv(busco_tsv, sep='\\t')\n",
    "busco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quast_df = pd.read_csv(quast_tsv, sep='\\t')\n",
    "quast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will therefore use the MaxBin2 results for the rest of the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the bin files\n",
    "bin_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(bin_dir):\n",
    "    bin_files.extend([os.path.join(dirpath, file) for file in filenames if file.endswith('.gz')])\n",
    "\n",
    "bin_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the MAGs (i.e. contigs and bins) from the bin files into dataframes\n",
    "bin_dfs = []\n",
    "\n",
    "for bin_file_gz in bin_files:\n",
    "\n",
    "    # Decompress (if they aren't already), then load the bin files as a dict {'node_id': ['length', 'cov', 'seq']} and convert to a dataframe\n",
    "    bin_file = decompress_gz(bin_file_gz)\n",
    "    bin_dict = {int(record.id.split('_')[1]): [int(record.id.split('_')[3]), float(record.id.split('_')[5]), str(record.seq)] for record in SeqIO.parse(bin_file, 'fasta')}\n",
    "    bin_df = pd.DataFrame.from_dict(bin_dict, orient='index', columns=['length', 'coverage', 'seq'])\n",
    "\n",
    "    # Add the bin ID to the dataframe\n",
    "    bin_df['bin_id'] = bin_file.split('/')[-1].split('.')[1]\n",
    "\n",
    "    # Add the dataframe to the list\n",
    "    bin_dfs.append(bin_df)\n",
    "\n",
    "# Concatenate all of the dataframes\n",
    "bin_df = pd.concat(bin_dfs).sort_index()\n",
    "bin_df = bin_df[~bin_df.index.duplicated(keep='first')]\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Taxonomic classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Taxonomic classification of trimmed reads\n",
    "\n",
    "The [kraken2](https://github.com/DerrickWood/kraken2/wiki/Manual) tool was used to classify trimmed reads using the [prebuilt 8GB minikraken DB](https://zenodo.org/record/4024003#.Y4-9PdLMK0o) as provided by the Center for Computational Biology of the John Hopkins University (from 2020-03). (**Note:** Using a larger kraken database would likely improve the results by decreasing the number of unclassified reads). The outputs were tab seperated files with the following columns:  \n",
    "\n",
    "1. `percentage` - Percentage of fragments covered by the clade rooted at this taxon\n",
    "2. `num_fragments` - Number of fragments covered by the clade rooted at this taxon\n",
    "3. `num_assigned` - Number of fragments assigned directly to this taxon\n",
    "4. `rank_code` - A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank.\n",
    "5. `tax_id` NCBI taxonomic ID number\n",
    "6. `scientific_name` - Indented scientific name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kraken_dfs = []\n",
    "names = ['percentage', 'num_fragments', 'num_assigned', 'rank_code', 'tax_id', 'scientific_name']\n",
    "\n",
    "for sample in samples:\n",
    "    kraken_file = f'{kraken_dir}/{sample}/kraken2_report.txt'\n",
    "    kraken_df = pd.read_csv(kraken_file, sep='\\t', header=None, names=names, index_col=0)\n",
    "    kraken_df['sample_id'] = sample\n",
    "    kraken_dfs.append(kraken_df)\n",
    "\n",
    "kraken_df = pd.concat(kraken_dfs)\n",
    "\n",
    "# Remove whitespace from the scientific name\n",
    "kraken_df.scientific_name = kraken_df.scientific_name.str.strip()\n",
    "kraken_df = kraken_df.reset_index()\n",
    "kraken_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define vars\n",
    "cols = ['total_raw_reads', 'total_processed_reads', 'classified_reads', 'unclassified_reads', 'bacterial_reads', 'viral_reads']\n",
    "display_as_percent = True\n",
    "vmax = None\n",
    "\n",
    "# Generate the individual dataframes\n",
    "unclass_df = kraken_df[kraken_df['rank_code'] == 'U'].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'unclassified_reads'})\n",
    "class_df = kraken_df[kraken_df['rank_code'] == 'R'].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'classified_reads'})\n",
    "bact_df = kraken_df[kraken_df['scientific_name'].str.contains('Bacteria')].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'bacterial_reads'})\n",
    "viral_df = kraken_df[kraken_df['scientific_name'].str.contains('Viruses')].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'viral_reads'})\n",
    "\n",
    "# Combine the dataframes and then process\n",
    "summary_df = pd.concat([unclass_df, class_df, bact_df, viral_df], axis=1)\n",
    "summary_df['total_raw_reads'] = [sample_read_counts[sample] for sample in summary_df.index]\n",
    "summary_df['total_processed_reads'] = summary_df['unclassified_reads'] + summary_df['classified_reads']\n",
    "summary_df['total_raw_reads'] = summary_df['total_raw_reads'].astype(int)\n",
    "summary_df['total_processed_reads'] = summary_df['total_processed_reads'].astype(int)\n",
    "summary_df.loc['total'] = summary_df.sum()\n",
    "summary_df = summary_df[cols]\n",
    "\n",
    "# Calculate percentages\n",
    "if display_as_percent:\n",
    "    vmax = 100\n",
    "    for col in cols[2:]:\n",
    "        cols[cols.index(col)] = f'{col} (%)'\n",
    "        summary_df = summary_df.rename(columns={col: f'{col} (%)'})\n",
    "        col = f'{col} (%)'\n",
    "        summary_df[col] = summary_df[col] / summary_df['total_processed_reads'] * 100\n",
    "\n",
    "# Display the summary dataframe with bars\n",
    "summary_df.style\\\n",
    "    .bar(subset=cols[:2], color='#d65f5f')\\\n",
    "    .bar(subset=cols[2:], color='#5fba7d', vmax=vmax)\\\n",
    "    .format('{:,.0f}', subset=cols[:2])\\\n",
    "    .format('{:.1f}', subset=cols[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Display the top 50 most abundant taxa\n",
    "# kraken_df = kraken_df[kraken_df.rank_code != 'U']\n",
    "# kraken_df = kraken_df[kraken_df.rank_code != 'R']\n",
    "# kraken_df = kraken_df.sort_values('num_fragments', ascending=False)\n",
    "# kraken_df.head(50)\n",
    "\n",
    "# Generate a Krona chart for all of samples - this is a bit hacky\n",
    "# !docker run -v $PWD:$PWD -w $PWD quay.io/biocontainers/krona:2.7.1--pl526_5 ktUpdateTaxonomy.sh taxonomy && !ktImportTaxonomy SRR14530762/kraken2_report.txt SRR14530763/kraken2_report.txt SRR14530764/kraken2_report.txt SRR14530765/kraken2_report.txt SRR14530766/kraken2_report.txt SRR14530767/kraken2_report.txt SRR14530769/kraken2_report.txt SRR14530770/kraken2_report.txt SRR14530771/kraken2_report.txt SRR14530772/kraken2_report.txt SRR14530880/kraken2_report.txt SRR14530881/kraken2_report.txt SRR14530882/kraken2_report.txt SRR14530884/kraken2_report.txt SRR14530885/kraken2_report.txt SRR14530886/kraken2_report.txt SRR14530887/kraken2_report.txt SRR14530888/kraken2_report.txt SRR14530889/kraken2_report.txt SRR14530890/kraken2_report.txt SRR14530891/kraken2_report.txt -tax taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Taxonomic classification of assembiles\n",
    "\n",
    "##### Virus classification of contigs\n",
    "\n",
    "Let's load the virus classifications predicted using [geNomad](https://github.com/apcamargo/genomad) and combine this with our existing data for the contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the virus classification data\n",
    "genomad_class_df = pd.read_csv(genomad_class, sep='\\t')\n",
    "genomad_tax_df = pd.read_csv(genomad_tax, sep='\\t')\n",
    "vir_df = pd.merge(genomad_class_df, genomad_tax_df, on='seq_name', how='left')\n",
    "vir_df.index = vir_df.seq_name.str.split('_').str[1].astype(int)\n",
    "vir_df = vir_df.drop('seq_name', axis=1)\n",
    "vir_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge the binning and taxonomy dataframes\n",
    "df = pd.merge(bin_df, vir_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Bacterial classification of MAGs\n",
    "\n",
    "Load the GTDB-Tk summary table (see [column descriptions](https://ecogenomics.github.io/GTDBTk/files/summary.tsv.html)) and combine with the existng information for the contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the GTDB-Tk classification data\n",
    "gtdbtk_df = pd.read_csv(gtdbtk_tsv, sep='\\t')\n",
    "\n",
    "# Filter the GTDB-Tk dataframe\n",
    "gtdbtk_df = gtdbtk_df[gtdbtk_df.user_genome.str.contains(binner)]\n",
    "cols = ['user_genome', 'classification', 'classification_method', 'other_related_references(genome_id,species_name,radius,ANI,AF)', 'msa_percent', 'red_value', 'warnings']\n",
    "gtdbtk_df = gtdbtk_df[cols]\n",
    "gtdbtk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add the GTDB-Tk classification data to the main dataframe\n",
    "gtdbtk_df['bin_id'] = gtdbtk_df.user_genome.str.split('.').str[1]\n",
    "gtdbtk_df = gtdbtk_df.drop('user_genome', axis=1)\n",
    "\n",
    "# Merge the GTDB-Tk dataframe with the main dataframe\n",
    "df = pd.merge(df, gtdbtk_df, on='bin_id', how='left')\n",
    "\n",
    "# Return index to how it was before\n",
    "df.index = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "This is what the final dataframe containing the information for each contig looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Explore/plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the distribution of contig lengths?\n",
    "\n",
    "Here, we can see that for contigs that are at least 500bp long, as their length increases, the number of contigs decreases superexponentially (note: the y-axis is shown as a log scale). \n",
    "\n",
    "The majority of contigs are less than 1000bp long and the longest contig is ~430kbp long, almost double the length of the next longest contig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to keep contigs that are at least 500 bp long\n",
    "contig_df = df[df['length'] >= 500].copy()\n",
    "contig_df['index'] = contig_df.index\n",
    "\n",
    "# Generate an interactive plot of the contig length distribution\n",
    "# Plot the contig ID (x-axis) against the contig length (y-axis)\n",
    "# Use a log scale for the y-axis so that the contigs are more easily visible\n",
    "\n",
    "px.defaults.template = 'plotly_white'\n",
    "fig = px.scatter(contig_df, x='index', y='length', log_y=True, hover_data={'length': ':,.0f', 'index': ':,.0f'}, width=1000, height=600)\n",
    "fig.update_xaxes(title_text='Contig ID')\n",
    "fig.update_yaxes(title_text='Contig length (bp)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the distribution of total base pairs for each of the contigs?\n",
    "\n",
    "For each contig, we have the average coverage ($\\overline{cov}$) which is equal to:\n",
    "\n",
    "$ \\overline{cov} = \\frac{read\\_len \\times n\\_reads}{contig\\_len} = \\frac{n\\_base\\_pairs}{contig\\_len} $\n",
    "\n",
    "Where:\n",
    "- $read\\_len$ = the average length of the reads\n",
    "- $n\\_reads$ = the number of reads (sequence fragments) that exactly align to the contig\n",
    "- $contig\\_len$ = the length of the contig\n",
    "- $n\\_base\\_pairs$ = the number of base pairs from reads that exactly align to the contig\n",
    "\n",
    "<br >\n",
    "\n",
    "We will be estimating the number of base pairs from the reads which map to the contig (which can be thought of as the contig weight) using the following equation:\n",
    "\n",
    "$ n\\_base\\_pairs = \\overline{cov} \\times contig\\_len$\n",
    "\n",
    "<br >\n",
    "\n",
    "We will then plot the distribution of the total number of base pairs for each contig similiar to the plot above. In doisng so, we can see that the contigs with the highest number of base pairs tend to be longer. However, for any given contig length, there is a wide range of total base pairs because the coverage varies across the contigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_df['n_base_pairs'] = contig_df['length'] * contig_df['coverage']\n",
    "\n",
    "fig = px.scatter(contig_df, x='index', y='n_base_pairs', log_x=False, log_y=True, hover_data={'index': ':,.0f', 'length': ':,.0f',  'coverage': ':,.0f', 'n_base_pairs': ':,.0f'}, width=1000, height=600, color='length')\n",
    "# fig = px.scatter(contig_df, x='length', y='n_base_pairs', log_x=True, log_y=True, hover_data={'index': ':,.0f', 'length': ':,.0f',  'coverage': ':,.0f', 'n_base_pairs': ':,.0f'}, width=1000, height=600, color='length')\n",
    "fig.update_xaxes(title_text='Contig ID')\n",
    "fig.update_yaxes(title_text='Total number of base pairs that align to the contig (bp)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each contig, how many reads are from each sample?\n",
    "\n",
    "Plot the number of reads for each contig coloured by the sample IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "rcParams['figure.figsize'] = 15, 8\n",
    "\n",
    "# n_samples = len(counts_df.columns)\n",
    "# cm = plt.get_cmap('gist_rainbow')\n",
    "# colors = [cm(1.*i/n_samples) for i in range(n_samples)]\n",
    "\n",
    "# This is a bit hacky as this is actually a line plot but I'm unsure of a better way to do this that is both informative and efficient given the amount of data\n",
    "ax = counts_df.plot(xlabel='Contig', ylabel='Number of mapped reads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate a heatmap for the N longest contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 85\n",
    "top_contigs_df = counts_df.head(n)\n",
    "\n",
    "# Generate a list of contig labels consisting of the contig ID, the prediction, bin and taxonomic classification\n",
    "top_contig_labels = []\n",
    "for index, row in df.head(n).iterrows():\n",
    "    # index\n",
    "    # Get the index of the max value in the scores\n",
    "    predictions = ['chromosome', 'plasmid', 'virus']\n",
    "    scores = [row.chromosome_score, row.plasmid_score, row.virus_score]\n",
    "    max_score = max(scores)\n",
    "    max_index = scores.index(max_score)\n",
    "    prediction = predictions[max_index]\n",
    "    bin = ''\n",
    "    if row.bin_id != 'NA':\n",
    "        bin = f'bin_{row.bin_id}'\n",
    "    if prediction != 'virus':\n",
    "        tax = str(row.classification).split(';')[-2].split('__')[-1]\n",
    "    else:\n",
    "        tax = str(row.lineage).split(';')[-1]\n",
    "    top_contig_labels.append(f'{index} {prediction} {bin} {tax}')\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "ax =sns.heatmap(top_contigs_df, cmap='viridis', cbar_kws={'label': 'Number of mapped reads'}, yticklabels=top_contig_labels)\n",
    "ax.set_xlabel('Sample ID')\n",
    "ax.set_ylabel('Contig ID')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What percentage of the reads were used in the assembly?\n",
    "Using the total number of processed reads for each sample and the counts matrix, we can calculate the percentage of reads used in the assembly for each sample.\n",
    "\n",
    "The results are promising, because for most of the samples there is a high % use of reads in the assembly. However, as noted in the [samtools documentation](http://www.htslib.org/doc/samtools-idxstats.html) the method we used to calculate the number of reads \"may count reads multiple times if they are mapped more than once or in multiple fragments.\" (Note: this is also why the % use of reads in the assembly is over 100% for some samples). This means the results are likely an overestimate and we cannot be sure if it's the same reads that are found in multiple contigs or if they're unique reads that are found in multiple contigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of processed reads for each sample\n",
    "sample_processed_read_counts = summary_df['total_processed_reads'].to_dict()\n",
    "\n",
    "# Convert the number of reads mapped to each contig to a percentage of the total number of processed read for each sample\n",
    "counts_perc_df = counts_df.copy()\n",
    "for sample in counts_perc_df.columns:\n",
    "    counts_perc_df[sample] = counts_perc_df[sample] / sample_processed_read_counts[sample] * 100\n",
    "\n",
    "# Sum the percentage of reads mapped to each contig for each sample\n",
    "counts_perc_df = pd.DataFrame(counts_perc_df.sum(axis=0), columns=['% reads mapped to contigs'])\n",
    "\n",
    "# Plot a bar chart of the percentage of reads mapped to each contig for each sample using seaborn\n",
    "ax = sns.barplot(x=counts_perc_df.index, y='% reads mapped to contigs', data=counts_perc_df, color='steelblue')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel('Sample ID')\n",
    "ax.set_ylabel('Total reads that mapped to contigs (%)')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c71d42a0e06c490c9858db74c56ab08eaac94618dde2a8910e248a9491f2839f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
