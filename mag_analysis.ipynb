{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAG Analysis\n",
    "\n",
    "Data from the [Rothman study](https://doi.org/10.1128/AEM.01448-21) corresponding to (unenriched) samples from the [HTP site](https://en.wikipedia.org/wiki/Hyperion_sewage_treatment_plant) was downloaded from the [ENA](https://www.ebi.ac.uk/ena/browser/view/prjna729801) and processed using the [nf-core/mag](https://github.com/PhilPalmer/mag/tree/genomad) pipeline to generate assembled and binned metagenomes (as shown by the image below). Here, we will analyse the results generated by the pipeline. First, we will load and aggregate the pipeline results, explaining the steps along the way. Then, we will use the aggregated results to answer some key questions about the data such as what organisms are present in the samples and how do the taxonomic classifications generated using the reads and the assembled genomes compare.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nf-core/mag/master/docs/images/mag_workflow.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "from Bio import SeqIO\n",
    "from matplotlib import rcParams\n",
    "# from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Change\n",
    "########\n",
    "# TODO: Upload directory to AWS S3 and add code to download the data if needed\n",
    "data_dir = '../mag/results_rothman_htp'\n",
    "\n",
    "##############\n",
    "# Don't change\n",
    "##############\n",
    "\n",
    "# QC\n",
    "fastqc_yaml = f'{data_dir}/multiqc/multiqc_data/multiqc_fastqc.yaml'\n",
    "\n",
    "# Assembly data\n",
    "bam_dir = f'{data_dir}/Assembly/SPAdes/QC/group-HTP'\n",
    "\n",
    "# Genome Binning data\n",
    "binner = 'MaxBin2'\n",
    "busco_tsv = f'{data_dir}/GenomeBinning/QC/busco_summary.tsv'\n",
    "quast_tsv = f'{data_dir}/GenomeBinning/QC/quast_summary.tsv'\n",
    "bin_tsv = f'{data_dir}/GenomeBinning/bin_summary.tsv'\n",
    "bin_dir = f'{data_dir}/GenomeBinning/{binner}'\n",
    "\n",
    "# Taxonomy data\n",
    "kraken_dir = f'{data_dir}/Taxonomy/kraken2'\n",
    "genomad_dir = f'{data_dir}/Taxonomy/geNomad'\n",
    "genomad_class = f'{genomad_dir}/group-HTP/SPAdes-group-HTP_scaffolds_aggregated_classification/SPAdes-group-HTP_scaffolds_aggregated_classification.tsv'\n",
    "genomad_tax = f'{genomad_dir}/group-HTP/SPAdes-group-HTP_scaffolds_annotate/SPAdes-group-HTP_scaffolds_taxonomy.tsv'\n",
    "gtdbtk_tsv = f'{data_dir}/Taxonomy/GTDB-Tk/gtdbtk_summary.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def decompress_gz(gz_file):\n",
    "    \"\"\"\n",
    "    Decompress a gzipped file.\n",
    "    \"\"\"\n",
    "    file = gz_file.replace('.gz', '')\n",
    "    if os.path.exists(gz_file) and not os.path.exists(file):\n",
    "        !gzip -dk {gz_file}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will load the data from the key steps of the pipeline in the rough order they ran.\n",
    "\n",
    "We will load each contig and associate it with any relevant information such as the length, bin/MAG and coverage etc. As we have information at different levels (sample, assembly, MAG and taxonomic levels) we will use IDs to link information in different dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Quality Control (QC)\n",
    "The first step of the pipeline was to perform QC on the raw reads. Here we will load the data from [FastQC](https://github.com/s-andrews/FastQC)/[MultiQC](https://github.com/ewels/MultiQC) to obtain the sample names and the total number of raw reads per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read the QC data\n",
    "with open(fastqc_yaml, 'r') as f:\n",
    "    fastqc_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Get the sample IDs and the total number of raw reads\n",
    "sample_read_counts = {k: fastqc_data[k]['Total Sequences'] for k in fastqc_data.keys()}\n",
    "sample_read_counts = {k.replace('_1',''): int(sample_read_counts[k] + sample_read_counts[k.replace('_1', '_2')]) for k in sample_read_counts.keys() if '_1' in k}\n",
    "samples = list(sample_read_counts.keys())\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Assemblies\n",
    "\n",
    "[metaSPAdes](http://cab.spbu.ru/software/spades/) was used for the assembly i.e. merging of reads.\n",
    "\n",
    "(We will skip loading the contigs here because instead, we will load the MAGs (in the next step) which includes the same information for the contigs and additional information such as the bin).\n",
    "\n",
    "#### Getting read counts for the contigs and samples\n",
    "\n",
    "Here we will generate a count matrix i.e. a file mapping contig IDs (rows) and sample IDs (columns).\n",
    "\n",
    "The nf-core/mag pipeline already maps the reads back to the contigs using [bowtie2](https://github.com/BenLangmead/bowtie2) (see [code](https://github.com/nf-core/mag/blob/master/modules/local/bowtie2_assembly_align.nf#L23-L30)). Therefore, we can use [samtools](http://www.htslib.org/) to get the read counts for each contig and sample (see [example](https://github.com/edamame-course/Metagenome/blob/master/2018-06-29-counting-abundance-with-mapped-reads.md#option-1-count-each-contigs)). The third column in the tab-seperated output from [`samtools idxstats`](http://www.htslib.org/doc/samtools-idxstats.html) is the number of reads mapped to the contig which we'll use to generate the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get absolute path for bam_dir and generate a list of all of the bams\n",
    "bam_dir = os.path.abspath(bam_dir)\n",
    "bams = [f'{bam_dir}/{bam}' for bam in os.listdir(bam_dir) if bam.endswith('.bam')]\n",
    "\n",
    "# Check if samtools is installed locally, if not, use the docker container from the nf-core/mag pipeline to run samtools\n",
    "if shutil.which('samtools'):\n",
    "    samtools_cmd = 'samtools'\n",
    "else:\n",
    "    samtools_cmd = f'docker run -v {bam_dir}:{bam_dir} -w {bam_dir} quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0 samtools'\n",
    "\n",
    "# Run samtools idxstats on all of the bams\n",
    "for bam in bams:\n",
    "    sample = bam.split('/')[-1].split('.')[0].split('-')[-1]\n",
    "    if not os.path.exists(f'{bam_dir}/{sample}_idxstats.txt'):\n",
    "        !{samtools_cmd} idxstats {bam} > {bam_dir}/{sample}_idxstats.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load and concatenate the idxstats files\n",
    "idxstat_cols = ['contig', 'length', 'num_mapped_reads', 'num_unmapped_reads']\n",
    "idxstat_dfs = []\n",
    "\n",
    "# Get a list of all of the idxstats files\n",
    "idxstats = [f'{bam_dir}/{idxstat}' for idxstat in os.listdir(bam_dir) if idxstat.endswith('_idxstats.txt')]\n",
    "\n",
    "# Read in the idxstats files\n",
    "for idxstat in idxstats:\n",
    "    sample = idxstat.split('/')[-1].split('_')[0]\n",
    "    idxstat_df = pd.read_csv(idxstat, sep='\\t', header=None, names=idxstat_cols)\n",
    "    idxstat_df['sample_id'] = sample\n",
    "    idxstat_dfs.append(idxstat_df)\n",
    "\n",
    "# Combine the idxstats dataframes into a single dataframe\n",
    "counts_df = pd.concat(idxstat_dfs, axis=0)\n",
    "counts_df.contig = counts_df.contig.str.split('_').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reformat the dataframe so that the contigs are the index and the samples are the columns\n",
    "counts_df = counts_df.pivot(index='contig', columns='sample_id', values='num_mapped_reads')\n",
    "\n",
    "# Sort the dataframe\n",
    "counts_df = counts_df[~counts_df.index.isnull()]\n",
    "counts_df.index = counts_df.index.astype(int)\n",
    "counts_df = counts_df.sort_index()\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Genome binning\n",
    "\n",
    "Here we will load the bins/MAGs\n",
    "\n",
    "- Two tools were used to perform metagenome binning to generate metagenome assembled genomes (MAGs) - [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/) and [MaxBin2](https://sourceforge.net/projects/maxbin2/)\n",
    "\n",
    "- And two tools were used to check for quality control (QC) of the genome bins - [Busco](https://busco.ezlab.org/) and [Quast](http://quast.sourceforge.net/quast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's look the summary information of the bins/MAGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "bin_summary_df = pd.read_csv(bin_tsv, sep='\\t', index_col=0)\n",
    "bin_summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results from the tools show that MaxBin2 generated more genome bins and a higher % completeness than MetaBAT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "busco_df = pd.read_csv(busco_tsv, sep='\\t')\n",
    "busco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "quast_df = pd.read_csv(quast_tsv, sep='\\t')\n",
    "quast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will therefore use the MaxBin2 results for the rest of the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the bin files\n",
    "bin_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(bin_dir):\n",
    "    bin_files.extend([os.path.join(dirpath, file) for file in filenames if file.endswith('.gz')])\n",
    "\n",
    "bin_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the MAGs (i.e. contigs and bins) from the bin files into dataframes\n",
    "bin_dfs = []\n",
    "\n",
    "for bin_file_gz in bin_files:\n",
    "\n",
    "    # Decompress (if they aren't already), then load the bin files as a dict {'node_id': ['length', 'cov', 'seq']} and convert to a dataframe\n",
    "    bin_file = decompress_gz(bin_file_gz)\n",
    "    bin_dict = {int(record.id.split('_')[1]): [int(record.id.split('_')[3]), float(record.id.split('_')[5]), str(record.seq)] for record in SeqIO.parse(bin_file, 'fasta')}\n",
    "    bin_df = pd.DataFrame.from_dict(bin_dict, orient='index', columns=['length', 'coverage', 'seq'])\n",
    "\n",
    "    # Add the bin ID to the dataframe\n",
    "    bin_df['bin_id'] = bin_file.split('/')[-1].split('.')[1]\n",
    "\n",
    "    # Add the dataframe to the list\n",
    "    bin_dfs.append(bin_df)\n",
    "\n",
    "# Concatenate all of the dataframes\n",
    "bin_df = pd.concat(bin_dfs).sort_index()\n",
    "bin_df = bin_df[~bin_df.index.duplicated(keep='first')]\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Taxonomic classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Taxonomic classification of trimmed reads\n",
    "\n",
    "The [kraken2](https://github.com/DerrickWood/kraken2/wiki/Manual) tool was used to classify trimmed reads using the [prebuilt 8GB minikraken DB](https://zenodo.org/record/4024003#.Y4-9PdLMK0o) as provided by the Center for Computational Biology of the John Hopkins University (from 2020-03). (**Note:** Using a larger kraken database would likely improve the results by decreasing the number of unclassified reads). The outputs were tab seperated files with the following columns:  \n",
    "\n",
    "1. `percentage` - Percentage of fragments covered by the clade rooted at this taxon\n",
    "2. `num_fragments` - Number of fragments covered by the clade rooted at this taxon\n",
    "3. `num_assigned` - Number of fragments assigned directly to this taxon\n",
    "4. `rank_code` - A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank.\n",
    "5. `tax_id` NCBI taxonomic ID number\n",
    "6. `scientific_name` - Indented scientific name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kraken_dfs = []\n",
    "names = ['percentage', 'num_fragments', 'num_assigned', 'rank_code', 'tax_id', 'scientific_name']\n",
    "\n",
    "for sample in samples:\n",
    "    kraken_file = f'{kraken_dir}/{sample}/kraken2_report.txt'\n",
    "    kraken_df = pd.read_csv(kraken_file, sep='\\t', header=None, names=names, index_col=0)\n",
    "    kraken_df['sample_id'] = sample\n",
    "    kraken_dfs.append(kraken_df)\n",
    "\n",
    "kraken_df = pd.concat(kraken_dfs)\n",
    "\n",
    "# Remove whitespace from the scientific name\n",
    "kraken_df.scientific_name = kraken_df.scientific_name.str.strip()\n",
    "kraken_df = kraken_df.reset_index()\n",
    "kraken_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define vars\n",
    "cols = ['total_raw_reads', 'total_processed_reads', 'classified_reads', 'unclassified_reads', 'bacterial_reads', 'viral_reads']\n",
    "display_as_percent = True\n",
    "vmax = None\n",
    "\n",
    "# Generate the individual dataframes\n",
    "unclass_df = kraken_df[kraken_df['rank_code'] == 'U'].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'unclassified_reads'})\n",
    "class_df = kraken_df[kraken_df['rank_code'] == 'R'].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'classified_reads'})\n",
    "bact_df = kraken_df[kraken_df['scientific_name'].str.contains('Bacteria')].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'bacterial_reads'})\n",
    "viral_df = kraken_df[kraken_df['scientific_name'].str.contains('Viruses')].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'viral_reads'})\n",
    "\n",
    "# Combine the dataframes and then process\n",
    "summary_df = pd.concat([unclass_df, class_df, bact_df, viral_df], axis=1)\n",
    "summary_df['total_raw_reads'] = [sample_read_counts[sample] for sample in summary_df.index]\n",
    "summary_df['total_processed_reads'] = summary_df['unclassified_reads'] + summary_df['classified_reads']\n",
    "summary_df['total_raw_reads'] = summary_df['total_raw_reads'].astype(int)\n",
    "summary_df['total_processed_reads'] = summary_df['total_processed_reads'].astype(int)\n",
    "summary_df.loc['total'] = summary_df.sum()\n",
    "summary_df = summary_df[cols]\n",
    "\n",
    "# Calculate percentages\n",
    "if display_as_percent:\n",
    "    vmax = 100\n",
    "    for col in cols[2:]:\n",
    "        cols[cols.index(col)] = f'{col} (%)'\n",
    "        summary_df = summary_df.rename(columns={col: f'{col} (%)'})\n",
    "        col = f'{col} (%)'\n",
    "        summary_df[col] = summary_df[col] / summary_df['total_processed_reads'] * 100\n",
    "\n",
    "# Display the summary dataframe with bars\n",
    "summary_df.style\\\n",
    "    .bar(subset=cols[:2], color='#d65f5f')\\\n",
    "    .bar(subset=cols[2:], color='#5fba7d', vmax=vmax)\\\n",
    "    .format('{:,.0f}', subset=cols[:2])\\\n",
    "    .format('{:.1f}', subset=cols[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Display the top 50 most abundant taxa\n",
    "# kraken_df = kraken_df[kraken_df.rank_code != 'U']\n",
    "# kraken_df = kraken_df[kraken_df.rank_code != 'R']\n",
    "# kraken_df = kraken_df.sort_values('num_fragments', ascending=False)\n",
    "# kraken_df.head(50)\n",
    "\n",
    "# Generate a Krona chart for all of samples - this is a bit hacky\n",
    "# !docker run -v $PWD:$PWD -w $PWD quay.io/biocontainers/krona:2.7.1--pl526_5 ktUpdateTaxonomy.sh taxonomy && !ktImportTaxonomy SRR14530762/kraken2_report.txt SRR14530763/kraken2_report.txt SRR14530764/kraken2_report.txt SRR14530765/kraken2_report.txt SRR14530766/kraken2_report.txt SRR14530767/kraken2_report.txt SRR14530769/kraken2_report.txt SRR14530770/kraken2_report.txt SRR14530771/kraken2_report.txt SRR14530772/kraken2_report.txt SRR14530880/kraken2_report.txt SRR14530881/kraken2_report.txt SRR14530882/kraken2_report.txt SRR14530884/kraken2_report.txt SRR14530885/kraken2_report.txt SRR14530886/kraken2_report.txt SRR14530887/kraken2_report.txt SRR14530888/kraken2_report.txt SRR14530889/kraken2_report.txt SRR14530890/kraken2_report.txt SRR14530891/kraken2_report.txt -tax taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Taxonomic classification of assembiles\n",
    "\n",
    "##### Virus classification of contigs\n",
    "\n",
    "Let's load the virus classifications predicted using [geNomad](https://github.com/apcamargo/genomad) and combine this with our existing data for the contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the virus classification data\n",
    "genomad_class_df = pd.read_csv(genomad_class, sep='\\t')\n",
    "genomad_tax_df = pd.read_csv(genomad_tax, sep='\\t')\n",
    "vir_df = pd.merge(genomad_class_df, genomad_tax_df, on='seq_name', how='left')\n",
    "vir_df.index = vir_df.seq_name.str.split('_').str[1].astype(int)\n",
    "vir_df = vir_df.drop('seq_name', axis=1)\n",
    "vir_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge the binning and taxonomy dataframes\n",
    "df = pd.merge(bin_df, vir_df, left_index=True, right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Bacterial classification of MAGs\n",
    "\n",
    "Load the GTDB-Tk summary table (see [column descriptions](https://ecogenomics.github.io/GTDBTk/files/summary.tsv.html)) and combine with the existng information for the contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the GTDB-Tk classification data\n",
    "gtdbtk_df = pd.read_csv(gtdbtk_tsv, sep='\\t')\n",
    "\n",
    "# Filter the GTDB-Tk dataframe\n",
    "gtdbtk_df = gtdbtk_df[gtdbtk_df.user_genome.str.contains(binner)]\n",
    "cols = ['user_genome', 'classification', 'classification_method', 'other_related_references(genome_id,species_name,radius,ANI,AF)', 'msa_percent', 'red_value', 'warnings']\n",
    "gtdbtk_df = gtdbtk_df[cols]\n",
    "gtdbtk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add the GTDB-Tk classification data to the main dataframe\n",
    "gtdbtk_df['bin_id'] = gtdbtk_df.user_genome.str.split('.').str[1]\n",
    "gtdbtk_df = gtdbtk_df.drop('user_genome', axis=1)\n",
    "\n",
    "# Merge the GTDB-Tk dataframe with the main dataframe\n",
    "df = pd.merge(df, gtdbtk_df, on='bin_id', how='left')\n",
    "\n",
    "# Return index to how it was before\n",
    "df.index = df.index + 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Explore/plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each contig, how many reads are from each sample?\n",
    "\n",
    "Plot the number of reads for each contig coloured by the sample IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "rcParams['figure.figsize'] = 15, 8\n",
    "\n",
    "# n_samples = len(counts_df.columns)\n",
    "# cm = plt.get_cmap('gist_rainbow')\n",
    "# colors = [cm(1.*i/n_samples) for i in range(n_samples)]\n",
    "\n",
    "ax = counts_df.plot(xlabel='Contig', ylabel='Number of mapped reads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate a heatmap for the 50 longest contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_contigs_df = counts_df.head(100)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax =sns.heatmap(top_contigs_df, cmap='viridis', cbar_kws={'label': 'Number of mapped reads'}) # , 'shrink': 0.5, 'ticks': [0, 1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "ax.set_xlabel('Sample ID')\n",
    "ax.set_ylabel('Contig ID')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c71d42a0e06c490c9858db74c56ab08eaac94618dde2a8910e248a9491f2839f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
