{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAG Analysis\n",
    "\n",
    "Analysing assembled and binned metagenomes generated using the [nf-core/mag](https://github.com/PhilPalmer/mag/tree/genomad) pipeline on the [Rothman dataset](https://doi.org/10.1128/AEM.01448-21) with samples from the [HTP site](https://en.wikipedia.org/wiki/Hyperion_sewage_treatment_plant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Change\n",
    "########\n",
    "# TODO: Upload directory to AWS S3 and add code to download the data if needed\n",
    "data_dir = '../mag/results_rothman_htp'\n",
    "\n",
    "##############\n",
    "# Don't change\n",
    "##############\n",
    "\n",
    "# QC\n",
    "fastqc_yaml = f'{data_dir}/multiqc/multiqc_data/multiqc_fastqc.yaml'\n",
    "\n",
    "# Assembly data\n",
    "# spades_dir = f'{data_dir}/Assembly/SPAdes'\n",
    "# spades_fa_gz = f'{spades_dir}/SPAdes-group-HTP_scaffolds.fasta.gz'\n",
    "\n",
    "# Genome Binning data\n",
    "binner = 'MaxBin2'\n",
    "busco_tsv = f'{data_dir}/GenomeBinning/QC/busco_summary.tsv'\n",
    "quast_tsv = f'{data_dir}/GenomeBinning/QC/quast_summary.tsv'\n",
    "bin_tsv = f'{data_dir}/GenomeBinning/bin_summary.tsv'\n",
    "bin_dir = f'{data_dir}/GenomeBinning/{binner}'\n",
    "contig_depths_gz = f'{data_dir}/GenomeBinning/depths/contigs/SPAdes-group-HTP-depth.txt.gz'\n",
    "\n",
    "# Taxonomy data\n",
    "genomad_dir = f'{data_dir}/Taxonomy/geNomad'\n",
    "genomad_tsv = f'{genomad_dir}/group-HTP/SPAdes-group-HTP_scaffolds_aggregated_classification/SPAdes-group-HTP_scaffolds_aggregated_classification.tsv'\n",
    "gtdbtk_tsv = f'{data_dir}/Taxonomy/GTDB-Tk/gtdbtk_summary.tsv'\n",
    "kraken_dir = f'{data_dir}/Taxonomy/kraken2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def decompress_gz(gz_file):\n",
    "    \"\"\"\n",
    "    Decompress a gzipped file.\n",
    "    \"\"\"\n",
    "    file = gz_file.replace('.gz', '')\n",
    "    if os.path.exists(gz_file) and not os.path.exists(file):\n",
    "        !gzip -dk {gz_file}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load each contig with any relevant information eg the bin/MAG and coverage etc.\n",
    "\n",
    "Link the infomation at the sample, assembly, MAG and taxonomic levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Control (QC)\n",
    "Print the sample names and get the total number of raw reads per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary containing the raw number of reads for each sample\n",
    "\n",
    "# Read the QC data\n",
    "with open(fastqc_yaml, 'r') as f:\n",
    "    fastqc_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Get the sample IDs and the total number of raw reads\n",
    "sample_read_counts = {k: fastqc_data[k]['Total Sequences'] for k in fastqc_data.keys()}\n",
    "sample_read_counts = {k.replace('_1',''): int(sample_read_counts[k] + sample_read_counts[k.replace('_1', '_2')]) for k in sample_read_counts.keys() if '_1' in k}\n",
    "sample_read_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fastqc_yaml, 'r') as f:\n",
    "    fastqc_data = yaml.safe_load(f)\n",
    "\n",
    "samples = [sample.split('_')[0] for sample in list(fastqc_data.keys())]\n",
    "samples = sorted(list(set(samples)))\n",
    "print(samples)\n",
    "\n",
    "n_reads = []\n",
    "for sample in samples:\n",
    "    n_reads.append(fastqc_data[f'{sample}_1']['Total Sequences'] + fastqc_data[f'{sample}_2']['Total Sequences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemblies\n",
    "\n",
    "[metaSPAdes](http://cab.spbu.ru/software/spades/) was used for the assembly and outputs the contigs i.e. merged reads as well as the scaffolds i.e. contigs that have been merged together\n",
    "\n",
    "The commented code below loads the contigs, however, we will be skipping this step because the same information will be loaded using the MAGs instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decompress the FASTA file (if it hasn't already been decompressed)\n",
    "# spades_fa = decompress_gz(spades_fa_gz)\n",
    "\n",
    "# # Load the FASTA seqs and save them as a dict in the following format {'node_id': ['length', 'cov', 'seq']}\n",
    "# seqs_dict = {int(record.id.split('_')[1]): [int(record.id.split('_')[3]), float(record.id.split('_')[5]), str(record.seq)] for record in SeqIO.parse(spades_fa, 'fasta')}\n",
    "\n",
    "# # Create dataframe containing contigs info\n",
    "# seqs_df = pd.DataFrame.from_dict(seqs_dict, orient='index', columns=['length', 'coverage', 'seq'])\n",
    "\n",
    "# seqs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the bins/MAGs\n",
    "\n",
    "- Two tools were used to perform metagenome binning to generate metagenome assembled genomes (MAGs) - [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/) and [MaxBin2](https://sourceforge.net/projects/maxbin2/)\n",
    "\n",
    "- And two tools were used to check for quality control (QC) of the genome bins - [Busco](https://busco.ezlab.org/) and [Quast](http://quast.sourceforge.net/quast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look the summary information of the bins/MAGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_df = pd.read_csv(bin_tsv, sep='\\t')\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the tools show that MaxBin2 generated more genome bins and a higher % completeness than MetaBAT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busco_df = pd.read_csv(busco_tsv, sep='\\t')\n",
    "busco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quast_df = pd.read_csv(quast_tsv, sep='\\t')\n",
    "quast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will therefore use the MaxBin2 results for the rest of the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bin files\n",
    "bin_files = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(bin_dir):\n",
    "    bin_files.extend([os.path.join(dirpath, file) for file in filenames if file.endswith('.gz')])\n",
    "\n",
    "bin_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dfs = []\n",
    "\n",
    "for bin_file_gz in bin_files:\n",
    "\n",
    "    # Load the bin file and save as a dataframe\n",
    "    bin_file = decompress_gz(bin_file_gz)\n",
    "    bin_dict = {int(record.id.split('_')[1]): [int(record.id.split('_')[3]), float(record.id.split('_')[5]), str(record.seq)] for record in SeqIO.parse(bin_file, 'fasta')}\n",
    "    bin_df = pd.DataFrame.from_dict(bin_dict, orient='index', columns=['length', 'coverage', 'seq'])\n",
    "\n",
    "    # Add the bin ID to the dataframe\n",
    "    bin_df['bin_id'] = bin_file.split('/')[-1].split('.')[1]\n",
    "\n",
    "    # Add the dataframe to the list\n",
    "    bin_dfs.append(bin_df)\n",
    "\n",
    "# Concatenate all of the dataframes\n",
    "bin_df = pd.concat(bin_dfs).sort_index()\n",
    "bin_df = bin_df[~bin_df.index.duplicated(keep='first')]\n",
    "bin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virus classification\n",
    "\n",
    "Let's load the virus classifications predicted using [geNomad](https://github.com/apcamargo/genomad) and combine this with our existing data for the contigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the virus classification data\n",
    "vir_df = pd.read_csv(genomad_tsv, sep='\\t')\n",
    "vir_df.index = vir_df.seq_name.str.split('_').str[1].astype(int)\n",
    "vir_df = vir_df.drop('seq_name', axis=1)\n",
    "vir_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the binning and taxonomy dataframes\n",
    "df = pd.merge(bin_df, vir_df, left_index=True, right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomic classification of binned genomes\n",
    "\n",
    "Load the GTDB-Tk summary table (see [column descriptions](https://ecogenomics.github.io/GTDBTk/files/summary.tsv.html)) and combine with the existng information for the contigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GTDB-Tk classification data\n",
    "gtdbtk_df = pd.read_csv(gtdbtk_tsv, sep='\\t')\n",
    "\n",
    "# Filter the GTDB-Tk dataframe\n",
    "gtdbtk_df = gtdbtk_df[gtdbtk_df.user_genome.str.contains(binner)]\n",
    "cols = ['user_genome', 'classification', 'classification_method', 'other_related_references(genome_id,species_name,radius,ANI,AF)', 'msa_percent', 'red_value', 'warnings']\n",
    "gtdbtk_df = gtdbtk_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the GTDB-Tk classification data to the main dataframe\n",
    "gtdbtk_df['bin_id'] = gtdbtk_df.user_genome.str.split('.').str[1]\n",
    "gtdbtk_df = gtdbtk_df.drop('user_genome', axis=1)\n",
    "\n",
    "# Merge the GTDB-Tk dataframe with the main dataframe\n",
    "df = pd.merge(df, gtdbtk_df, on='bin_id', how='left')\n",
    "\n",
    "# Return index to how it was before\n",
    "df.index = df.index + 1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomic classification of trimmed reads\n",
    "\n",
    "The [kraken2](https://github.com/DerrickWood/kraken2/wiki/Manual) tool was used to classify trimmed reads using the [prebuilt 8GB minikraken DB](https://zenodo.org/record/4024003#.Y4-9PdLMK0o) as provided by the Center for Computational Biology of the John Hopkins University (from 2020-03). (**Note:** Using a larger kraken database would likely improve the results by decreasing the number of unclassified reads). The outputs were tab seperated files with the following columns:  \n",
    "\n",
    "1. `percentage` - Percentage of fragments covered by the clade rooted at this taxon\n",
    "2. `num_fragments` - Number of fragments covered by the clade rooted at this taxon\n",
    "3. `num_assigned` - Number of fragments assigned directly to this taxon\n",
    "4. `rank_code` - A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank.\n",
    "5. `tax_id` NCBI taxonomic ID number\n",
    "6. `scientific_name` - Indented scientific name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken_dfs = []\n",
    "names = ['percentage', 'num_fragments', 'num_assigned', 'rank_code', 'tax_id', 'scientific_name']\n",
    "\n",
    "for sample in samples:\n",
    "    kraken_file = f'{kraken_dir}/{sample}/kraken2_report.txt'\n",
    "    kraken_df = pd.read_csv(kraken_file, sep='\\t', header=None, names=names)\n",
    "    kraken_df['sample_id'] = sample\n",
    "    kraken_dfs.append(kraken_df)\n",
    "\n",
    "kraken_df = pd.concat(kraken_dfs)\n",
    "\n",
    "# Remove whitespace from the scientific name\n",
    "kraken_df.scientific_name = kraken_df.scientific_name.str.strip()\n",
    "kraken_df = kraken_df.reset_index()\n",
    "kraken_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vars\n",
    "cols = ['total_raw_reads', 'total_processed_reads', 'classified_reads', 'unclassified_reads', 'bacterial_reads', 'viral_reads']\n",
    "display_as_percent = True\n",
    "vmax = None\n",
    "\n",
    "# Generate the individual dataframes\n",
    "unclass_df = kraken_df[kraken_df['rank_code'] == 'U'].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'unclassified_reads'})\n",
    "class_df = kraken_df[kraken_df['rank_code'] == 'R'].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'classified_reads'})\n",
    "bact_df = kraken_df[kraken_df['scientific_name'].str.contains('Bacteria')].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'bacterial_reads'})\n",
    "viral_df = kraken_df[kraken_df['scientific_name'].str.contains('Viruses')].groupby('sample_id').agg({'num_fragments': 'sum'}).rename(columns={'num_fragments': 'viral_reads'})\n",
    "\n",
    "# Combine the dataframes and then process\n",
    "summary_df = pd.concat([unclass_df, class_df, bact_df, viral_df], axis=1)\n",
    "summary_df['total_raw_reads'] = [sample_read_counts[sample] for sample in summary_df.index]\n",
    "summary_df['total_processed_reads'] = summary_df['unclassified_reads'] + summary_df['classified_reads']\n",
    "summary_df['total_raw_reads'] = summary_df['total_raw_reads'].astype(int)\n",
    "summary_df['total_processed_reads'] = summary_df['total_processed_reads'].astype(int)\n",
    "summary_df.loc['total'] = summary_df.sum()\n",
    "summary_df = summary_df[cols]\n",
    "\n",
    "# Calculate percentages\n",
    "if display_as_percent:\n",
    "    vmax = 100\n",
    "    for col in cols[2:]:\n",
    "        cols[cols.index(col)] = f'{col} (%)'\n",
    "        summary_df = summary_df.rename(columns={col: f'{col} (%)'})\n",
    "        col = f'{col} (%)'\n",
    "        summary_df[col] = summary_df[col] / summary_df['total_processed_reads'] * 100\n",
    "\n",
    "# Display the summary dataframe with bars\n",
    "summary_df.style\\\n",
    "    .bar(subset=cols[:2], color='#d65f5f')\\\n",
    "    .bar(subset=cols[2:], color='#5fba7d', vmax=vmax)\\\n",
    "    .format('{:.2f}', subset=cols[2:])\\\n",
    "    .format('{:,.0f}', subset=cols[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the top 50 most abundant taxa\n",
    "# kraken_df = kraken_df[kraken_df.rank_code != 'U']\n",
    "# kraken_df = kraken_df[kraken_df.rank_code != 'R']\n",
    "# kraken_df = kraken_df.sort_values('num_fragments', ascending=False)\n",
    "# kraken_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Krona chart for all of samples - this is a bit hacky\n",
    "# !docker run -v $PWD:$PWD -w $PWD quay.io/biocontainers/krona:2.7.1--pl526_5 ktUpdateTaxonomy.sh taxonomy && !ktImportTaxonomy SRR14530762/kraken2_report.txt SRR14530763/kraken2_report.txt SRR14530764/kraken2_report.txt SRR14530765/kraken2_report.txt SRR14530766/kraken2_report.txt SRR14530767/kraken2_report.txt SRR14530769/kraken2_report.txt SRR14530770/kraken2_report.txt SRR14530771/kraken2_report.txt SRR14530772/kraken2_report.txt SRR14530880/kraken2_report.txt SRR14530881/kraken2_report.txt SRR14530882/kraken2_report.txt SRR14530884/kraken2_report.txt SRR14530885/kraken2_report.txt SRR14530886/kraken2_report.txt SRR14530887/kraken2_report.txt SRR14530888/kraken2_report.txt SRR14530889/kraken2_report.txt SRR14530890/kraken2_report.txt SRR14530891/kraken2_report.txt -tax taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get read counts for the MAGs\n",
    "\n",
    "### Get the sequencing depths per contig\n",
    "\n",
    "Load the sequencing depths per contig and sample. This file is roughly equivalent to a count matrix i.e. a file mapping contig IDs (rows) and sample SRA IDs (columns) with the number of reads mapping to each contig in each sample. However, instead of the number of reads, we have the sequencing depth i.e. the number of bases divided by the contig length\n",
    "\n",
    "This is calculated using MetaBAT2's `jgi_summarize_bam_contig_depths --outputDepth`. The values correspond to `(sum of exactly aligned bases) / ((contig length)-2*75)`. For example, for two reads aligned exactly with `10` and `9` bases on a 1000 bp long contig the depth is calculated by `(10+9)/(1000-2*75)` (1000bp length of contig minus 75bp from each end, which is excluded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the contig depths\n",
    "contig_depths = decompress_gz(contig_depths_gz)\n",
    "contig_depths_df = pd.read_csv(contig_depths, sep='\\t')\n",
    "\n",
    "# Preprocess the contig depths dataframe\n",
    "contig_depths_df.index = contig_depths_df.contigName.str.split('_').str[1].astype(int)\n",
    "contig_depths_df = contig_depths_df.drop(['contigName', 'contigLen'], axis=1)\n",
    "\n",
    "# Drop all columns containing 'var' (i.e. the columns containing the variance)\n",
    "contig_depths_df = contig_depths_df.drop([col for col in contig_depths_df.columns if 'var' in col], axis=1)\n",
    "\n",
    "# Rename the columns e.g. from `SPAdes-group-HTP-SRR14530771.bam` to `SRR14530771`\n",
    "contig_depths_df.columns = [col.split('-')[-1].split('.')[0] for col in contig_depths_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the contig depths with the main dataframe\n",
    "df = pd.merge(df, contig_depths_df, left_index=True, right_index=True, how='left')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c71d42a0e06c490c9858db74c56ab08eaac94618dde2a8910e248a9491f2839f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
